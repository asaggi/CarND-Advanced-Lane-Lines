<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>writeup_template</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<h2 id="toc_0">CarND-Advanced-Lane-Lines</h2>

<p><strong>Advanced Lane Finding Project</strong></p>

<p>Scripts:</p>

<ul>
<li><code>calib.py</code> - Scxript to display and save camera calibration</li>
<li><code>detect.py</code> - script to display and save detection pipeline</li>
</ul>

<p>The goals / steps of this project are the following:</p>

<ul>
<li>Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.</li>
<li>Apply a distortion correction to raw images.</li>
<li>Use color transforms, gradients, etc., to create a thresholded binary image.</li>
<li>Apply a perspective transform to rectify binary image (&quot;birds-eye view&quot;).</li>
<li>Detect lane pixels and fit to find the lane boundary.</li>
<li>Determine the curvature of the lane and vehicle position with respect to center.</li>
<li>Warp the detected lane boundaries back onto the original image.</li>
<li>Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.</li>
</ul>

<h3 id="toc_1">Camera Calibration</h3>

<h4 id="toc_2">1. Camera calibration is done in utils/calibrate.py. Script loads every image that match following pattern: <code>camera_cal/*.jpg.</code></h4>

<p>Termination criteria are defined as follows.<br>
<code>criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)</code></p>

<p>Here I am assuming the chessboard is fixed on the (6, 9) plane at z=0, such that the object points are the same for each calibration image.  Here I use 3D reference coordinates and arrays to store data (calibrate.py:11).
<code>objp = np.zeros((6*9,3), np.float32)</code></p>

<p>Then I iterate through all the images saved in <code>camera_cal/*.jpg</code> and try to find chess board on each. If found, I add points to each array and display. An image for reference to generate output.</p>

<p><img src="./camera_cal/calibration9.jpg" alt="alt text" title="Distorted">
<img src="./output_images/calib_example.jpg" alt="alt text" title="Undistorted"></p>

<p>Then we distort all windows and do actual calibration: (calibrate.py:40).</p>

<p><code>cv2.destroyAllWindows()</code></p>

<p>After that, this data is dumped into a JSON format for the next script (calibrate.py:51).</p>

<h3 id="toc_3">Pipeline (single images)</h3>

<p>Pipeline is done in utils/detect.py. Script loads every image that match following pattern: <code>test_images/*.jpg</code> and then loads project_video.mp4.</p>

<h4 id="toc_4">The pipeline for images or first frame of video is following  (utils/detect.py:348):</h4>

<ul>
<li>undistortion.<br></li>
<li>perspective transform</li>
<li>binarization</li>
<li>line fitting</li>
<li>calculate curvature and position</li>
</ul>

<h4 id="toc_5">1. Distortion correction</h4>

<p>Distortion is corrected using OpenCV <code>cv2.undistort</code> function (utils/detect.py:352) with provided camera calibration data (utils/detect.py:12). Images before and after distortion correction are shown below</p>

<p><img src="./output_images/distortion.jpg" alt="alt text" title="Distortion"></p>

<h4 id="toc_6">2. Perspective transform</h4>

<p>Perspective transform is done using <code>cv2.getPerspectiveTransform</code> and <code>cv2.warpPerspective</code> (utils/detect.py:229 - 230). Input points are defined as follows: (top left = 0,0, bottom right = 1,1)</p>

<ul>
<li>0.16, 1</li>
<li>0.45, 0.63</li>
<li>0.55, 0.63</li>
<li>0.84, 1</li>
</ul>

<p><img src="./output_images/persepective.jpg" alt="alt text" title="Perspective"></p>

<p>4 points used to compute transformation matrix.</p>

<p><img src="./output_images/mask.jpg" alt="alt text" title="Mask">
Images are masked above 0.63 and 10% from bottom (utils/detect.py:233).</p>

<p><img src="./output_images/transformed.jpg" alt="alt text" title="Transformed">.<br>
Image transformed to top view using warp perspective (utils/detect.py:337). </p>

<h4 id="toc_7">3. Binarization  (utils/detect.py:30)</h4>

<p>Binarization is done in following steps:</p>

<ul>
<li>Blur with 5x5 kernel using <code>cv2.filter2D</code> (utils/detect.py:38). </li>
<li>Equalize histogram (utils/detect.py:40)</li>
<li>Convert to YUV colorspace (utils/detect.py:41)</li>
<li>Use cv2.equalizeHist with Y channel (utils/detect.py:42)</li>
<li>Convert back to RGB (utils/detect.py:43)</li>
<li>Find white lines</li>
<li>Leave only G channel</li>
<li>Black all values below 250 and set rest to 255  (utils/detect.py:48)</li>
<li>Find yellow lines </li>
<li>Use V from YUV</li>
<li>Blur with 5x5</li>
<li>Apply sobel in x and threshold (utils/detect.py:56-57)</li>
<li>Merge results from 3 and 4 (utils/detect.py:60)</li>
</ul>

<p><img src="./output_images/binary.jpg" alt="alt text" title="Binary">.</p>

<h4 id="toc_8">4. Line fitting (utils/detect.py:63)</h4>

<p>Line fitting for single image or first frame of video works as follows:</p>

<ul>
<li>Set center = width/2.<br></li>
<li>For each line in image starting from bottom:

<ul>
<li>Store index of each none zero point in x<em>val</em>hist</li>
<li>Store in x<em>val</em>hist index of each none zero point from row</li>
<li>If len of x<em>val</em>hist is greater than 0:

<ul>
<li>Group points to left/right line according to position to center</li>
</ul></li>
<li>If there are points in left group:

<ul>
<li>Compute average</li>
</ul></li>
<li>Add point to points</li>
<li>Set center = average + width*0.2</li>
</ul></li>
<li>If there are points in left group:

<ul>
<li>Compute mean</li>
<li>Add point to points</li>
</ul></li>
</ul>

<p>Image below shows red, green and blue dots. Red and blue are points for left and right line fitting respectively. Green is the position of center which discriminates left from right points.</p>

<p><img src="./output_images/lines.jpg" alt="alt text" title="Lines"></p>

<h4 id="toc_9">5. Line fitting for a video frame (utils/detect.py:170):</h4>

<ul>
<li>For every 10th row of frame starting from bottom:

<ul>
<li>Compute min, max of ROI for x, for left and right lines to search for points (lxmin, lxmax, rxmin, rxmax)</li>
<li>Store in x<em>val</em>hist index of each none zero point from row</li>
<li>If len of x<em>val</em>hist is greater than 5:

<ul>
<li>Group points to left/right line according to position to lxmin, lxmax and rxmin, rxmax</li>
</ul></li>
<li>If there are points in left group:</li>
<li>Compute average</li>
<li>Add point to points</li>
<li>Set center = average + width*0.2</li>
<li>If there are points in left group:

<ul>
<li>Compute mean</li>
</ul></li>
<li>Add point to points</li>
<li>If there are less than 10 points in left or right line 25 times, single image line fitting is made.</li>
</ul></li>
</ul>

<p><img src="./output_images/lines-video.jpg" alt="alt text" title="Lines-video"></p>

<h4 id="toc_10">5. Calculating radius / curvature and position (utils/detect.py:244)</h4>

<p>All radius/position related computations are made in <code>computeAndShow</code> method . Curvature is computed using code from Udacity. Position is obtained by computing the difference between center of image and center of lane in pixels. Then it is converted to meters using pixels to meters factor for x axis.</p>

<h4 id="toc_11">6. Results</h4>

<p><img src="./output_images/test1.jpg" alt="alt text" title="Binary Example">
<img src="./output_images/test2.jpg" alt="alt text" title="Binary Example">
<img src="./output_images/test3.jpg" alt="alt text" title="Binary Example">
<img src="./output_images/test4.jpg" alt="alt text" title="Binary Example">
<img src="./output_images/test5.jpg" alt="alt text" title="Binary Example"></p>

<h2 id="toc_12"><img src="./output_images/test6.jpg" alt="alt text" title="Binary Example"></h2>

<h3 id="toc_13">Pipeline (video)</h3>

<p><code>project_video.mp4</code> is available in output_images directory.</p>

<hr>

<h3 id="toc_14">Discussion</h3>

<h4 id="toc_15">1. This solution works good on the project_video example, but was not able to perform well on <code>challenge</code> and <code>harder_challenge</code>.</h4>

<p>We can improve the detection by:</p>

<ul>
<li>Fine tunning of binarization</li>
<li>Support for small radius turns - larger ROI</li>
</ul>




</body>

</html>
